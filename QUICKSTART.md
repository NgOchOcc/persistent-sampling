# Quick Start Guide

## CÃ i Ä‘áº·t

### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies

```bash
cd /Users/luungoc/Qualcomm/Project/Reasoning/persistent_sampling

# CÃ i Ä‘áº·t dependencies cÆ¡ báº£n
pip install numpy tqdm pyyaml pandas datasets

# CÃ i Ä‘áº·t vLLM (yÃªu cáº§u GPU)
pip install vllm

# Hoáº·c náº¿u muá»‘n build tá»« source
pip install git+https://github.com/vllm-project/vllm.git
```

### BÆ°á»›c 2: Kiá»ƒm tra installation

```bash
# Cháº¡y basic tests (khÃ´ng cáº§n GPU)
python scripts/test_basic.py
```

## Sá»­ dá»¥ng nhanh

### 1. Test vá»›i sample data (khÃ´ng cáº§n dataset lá»›n)

```bash
# Cháº¡y trÃªn sample MATH problems
python scripts/run_math500.py \
    --use_sample \
    --N 8 \
    --max_steps 30 \
    --output_name sample_test.json
```

### 2. Cháº¡y demo Ä‘Æ¡n giáº£n

```bash
# Demo vá»›i 1 problem
python scripts/demo.py
```

### 3. Python API

```python
import sys
sys.path.append('src')

from persistent_smc import PersistentSMC
from vllm_wrapper import vLLMGenerator

# Khá»Ÿi táº¡o vLLM
llm = vLLMGenerator(
    model_name="Qwen/Qwen2.5-Math-7B-Instruct"
)

# Khá»Ÿi táº¡o Persistent SMC
solver = PersistentSMC(
    llm_generator=llm,
    N=16,                    # Sá»‘ particles
    k_max=10,                # Window size
    annealing_method="ess_targeted"
)

# Giáº£i bÃ i toÃ¡n
problem = "What is 15% of 80?"
prompt = llm.format_math_prompt(problem)
solutions = solver.solve(prompt, max_steps=30)

# Láº¥y káº¿t quáº£
from collections import Counter
from dataset_loaders import extract_final_answer_math

answers = [extract_final_answer_math(s.text) for s in solutions]
answers = [a for a in answers if a is not None]
final_answer = Counter(answers).most_common(1)[0][0] if answers else None

print(f"Answer: {final_answer}")
```

## Cháº¡y evaluation trÃªn datasets

### MATH500

```bash
# Download MATH dataset tá»« HuggingFace (tá»± Ä‘á»™ng)
python scripts/run_math500.py \
    --model Qwen/Qwen2.5-Math-7B-Instruct \
    --N 16 \
    --max_steps 50 \
    --num_problems 100 \
    --output_name math500_100.json
```

### AIME24

```bash
# Cáº§n prepare AIME24 dataset trÆ°á»›c (hoáº·c dÃ¹ng sample)
python scripts/run_aime24.py \
    --data_path data/aime24_sample.json \
    --model Qwen/Qwen2.5-Math-7B-Instruct \
    --N 24 \
    --max_steps 80 \
    --output_name aime24_test.json
```

## Hyperparameter tuning

### Cho easy problems (MATH Level 1-2):
```bash
python scripts/run_math500.py \
    --N 8 \
    --k_max 5 \
    --max_steps 30 \
    --temperature 0.7
```

### Cho hard problems (MATH Level 4-5, AIME):
```bash
python scripts/run_math500.py \
    --N 24 \
    --k_max 15 \
    --max_steps 80 \
    --temperature 0.8 \
    --difficulty "Level 5"
```

## Troubleshooting

### Lá»—i: ModuleNotFoundError: No module named 'vllm'

```bash
# CÃ i Ä‘áº·t vLLM
pip install vllm

# Náº¿u gáº·p lá»—i vá»›i CUDA
pip install vllm --no-build-isolation
```

### Lá»—i: CUDA out of memory

```bash
# Giáº£m sá»‘ particles
python scripts/run_math500.py --N 8 --k_max 5

# Hoáº·c sá»­ dá»¥ng model nhá» hÆ¡n
python scripts/run_math500.py \
    --model Qwen/Qwen2.5-Math-1.5B-Instruct
```

### Lá»—i: Dataset not found

```bash
# Sá»­ dá»¥ng sample data
python scripts/run_math500.py --use_sample

# Hoáº·c download tá»« HuggingFace (tá»± Ä‘á»™ng náº¿u cÃ³ datasets library)
pip install datasets
```

## Monitoring

Trong quÃ¡ trÃ¬nh cháº¡y, báº¡n sáº½ tháº¥y:

```
=== Step 5 ===
Alive particles: 16
ESS: 12.34 / 16 (77.13%)
Beta: 0.2341
SC scores: mean=0.123, std=0.045
```

**Chá»‰ sá»‘ quan trá»ng:**
- **ESS**: NÃªn > 30% cá»§a N_alive
- **Beta**: TÄƒng dáº§n tá»« 0 â†’ 1
- **SC scores**: Äo Ä‘á»™ tá»± tin cá»§a model

## Káº¿t quáº£

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong `results/` dÆ°á»›i dáº¡ng JSON:

```json
{
  "accuracy": 0.85,
  "correct_count": 85,
  "total": 100,
  "pass_at_k": {
    "pass@1": 0.82,
    "pass@8": 0.91
  },
  "results": [...]
}
```

## TÃ i liá»‡u chi tiáº¿t

- **LÃ½ thuyáº¿t Ä‘áº§y Ä‘á»§**: `PERSISTENT_SMC_FOR_LLM_REASONING.md`
- **Thuáº­t toÃ¡n chi tiáº¿t**: `algorithms.tex`
- **Code reference**: Comments trong cÃ¡c file `.py`

## Performance Tips

1. **Sá»­ dá»¥ng ESS-targeted annealing**: Tá»‘t nháº¥t cho háº§u háº¿t cases
2. **Enable prefix caching**: ÄÃ£ máº·c Ä‘á»‹nh trong vLLM wrapper
3. **Batch generation**: Tá»± Ä‘á»™ng trong implementation
4. **Monitor ESS**: Náº¿u ESS tháº¥p, tÄƒng `target_ess_ratio`

## Next Steps

1. **Thá»­ vá»›i model khÃ¡c**:
   - DeepSeek-Math
   - WizardMath
   - MetaMath

2. **TÃ¹y chá»‰nh hyperparameters**:
   - Chá»‰nh `N` vÃ  `k_max` theo Ä‘á»™ khÃ³
   - Thá»­ cÃ¡c annealing methods khÃ¡c
   - Äiá»u chá»‰nh temperature

3. **ThÃªm datasets**:
   - GSM8K
   - SVAMP
   - MathQA

4. **Integrate Process Reward Model**:
   - Thay tháº¿ self-certainty báº±ng PRM trained

ChÃºc báº¡n thÃ nh cÃ´ng vá»›i research! ğŸš€
